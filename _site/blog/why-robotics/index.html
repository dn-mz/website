<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"/>
        <title>Daniel</title>
        <meta name="description" content="Exited founder, former BigLaw attorney, Schwarzman Scholar, and robotics builder." />
    
        <link rel="stylesheet" href="/style.css">

        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-M8LEGE937H"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-M8LEGE937H');
        </script>

      </head>
<body>
    <nav class="nav">
        <div class="container nav-inner">
            <a href="/" class="brand">Daniel Ng ü¶æ</a>
            
            <button id="menuBtn" class="menu-button" aria-label="Toggle Menu">
              <svg viewBox="0 0 24 24" width="24" height="24" stroke="currentColor" stroke-width="2" fill="none">
                <line x1="3" y1="12" x2="21" y2="12"></line>
                <line x1="3" y1="6" x2="21" y2="6"></line>
                <line x1="3" y1="18" x2="21" y2="18"></line>
              </svg>
            </button>
    
            <div class="nav-links">
                <a class="nav-link" href="/about">About</a>
                <a class="nav-link" href="/blog">Reflections</a>
                <a class="nav-link" href="/book">Contact</a>
            </div>
        </div> 
    
        <div class="container mobile-menu" id="mobileMenu" style="display:none;">
            <a href="/about">About</a>
            <a href="/blog">Reflections</a>
            <a href="/book">Contact</a>
            <a href="www.ezer.work">Ezer</a>
        </div>
    </nav>

  <article class="container section">
  <header style="margin-bottom: 2rem;">
    <h1 style="margin-bottom: 0.5rem;">Why robotics</h1>
    <span style="color: var(--muted-fg);">December 31, 2025</span>
  </header>
  
  <hr style="border: 0; border-top: 1px solid var(--border); margin-bottom: 2rem;">

  <div class="post-content" style="line-height: 1.8; font-size: 1.1rem;">
    <p>Putting aside my team‚Äôs second-placing on day two of the 2003 National Junior Robotics Competition, my first deep look into robotics was after founding JustShip in 2020. At JustShip, we built a great tech-ops stack to streamline many operationally tedious processes for our customers, logistics partners and ourselves. Naturally, this involved looking at processes in the physical warehouse to automate with tech. Packing. Boxing. Retrieval. Sorting. Putting things in the right places. <em>Enter robotics.</em></p>

<p><strong><em>Robotics, then</em></strong></p>

<p>It was 2020 then, and robotics wasn‚Äôt yet quite ready. I looked into various options, but many didn‚Äôt make sense. Machines half the size of our meeting room; six-figure price tags with unit economics that don‚Äôt add up; tons of limitations and edge cases; and lots of manual intervention required still. The robots mostly worked, some of the time, for a few use cases; thus, there were not many ideal customers. It‚Äôs not to say robotics didn‚Äôt work at all in 2020. It just didn‚Äôt make sense, at least for JustShip then, if things didn‚Äôt work generally nor all the time.</p>

<p><strong><em>Robotics, now</em></strong></p>

<p>It‚Äôs 2025. Robotics isn‚Äôt yet quite ready, still. But massive developments in hardware have brought the cost of robots down to low four figures, and even greater developments in AI and software (no elaboration required here) have led to significant unlocks in robotics - specifically, AI robotics. I can go on about the specifics of the technology, promise of general purpose robotics, interesting open-source models, and fascinating behind-the-scenes work and whispers I‚Äôve heard. The important thing: the limitations I saw in industrial robotics in 2020 no longer exist with AI robotics now in 2025.</p>

<p>Don‚Äôt get me wrong - there are still plenty of limitations. There are many, many new challenges in the AI robotics space. Yet, AI robotics has ushered in a new type of robot work. Robots have reduced in size, prices gone down, can be generalisable, and now can (purportedly/potentially) perceive, think, and react.</p>

<p>I‚Äôm not going to go into specifics of the model, data or compete; nor my opinions on data, deployment or form factor. There have been many recent developments that have fascinated me. But I‚Äôll document those learnings over the next few weeks and months.</p>

<p>For now, I‚Äôm going to reflect on two things: <em>learning robots</em>, and <em>robot learning</em>.</p>

<p><strong><em>Learning robots</em></strong></p>

<p>Given my background, the question is: how am I supposed to learn robotics? Or, how did I learn robotics? I don‚Äôt have a degree in engineering (yet). I haven‚Äôt spent years mastering control systems or inverse kinematics (yet), and there is still much I don‚Äôt know about sensors (yet).</p>

<p>I entered the field by tinkering with a friend. He built the tech stack for teleoperation and a VLA (Vision-Language-Action) policy; I focused on scaling data, quality control, hardware maintenance, and managing operations. This threw me head-first into the challenge of robotics - the tricky, unglamorous bits like USB cable matters (especially with three cameras!), RAM latency, actuator calibration, and the complexities of leader-follower teleop set-ups.</p>

<p>Within two months, we saw incredible results. We built a data collection engine with 30 operators that generated high-diversity datasets, deployed across multiple real businesses in the UK (including an insect farm!) with more in the pipeline. Our models were performing at SOTA levels. I remember how excited we were when we saw our robots come to life, and improve with the regular iterations we had.</p>

<p>We went to SF to raise money, including flying our robots down to SF, received significant funding (almost $10M), but on the day the deal was to close, we decided not take it. It‚Äôs a much longer story and post-mortem here, for another time.</p>

<p>Since then, I‚Äôve focused on building my ‚Äúrobot intuition‚Äù. I‚Äôve built a mobile base for my robot arms to get my hands dirty with circuits, motors, and coding the software of the mobile base; trialled simulations and other collection methods (UMI); and set up various systems (GPUs, 3D printers, much more). There‚Äôs a lot happening in the robot data space right now, and many are trying to enter the space. I‚Äôve been really impressed by some approaches, slightly skeptical about others, and overall am focusing on learning robots deeply, to deliver results powerfully for the real world. On that note, many companies have reached out to me to chat and leverage my experience, and I‚Äôm happy to <a href="/book" class="custom-link">chat</a>.</p>

<p><strong><em>Robot learning</em></strong></p>

<p>Thus my thesis on robot learning. Putting aside traditional industrial robotics, and focusing on AI robotics, the space is quite fragmented much like the early LLM landscape. Companies tend to focus on individual parts of the process - hardware, software, research, data, labelling - and that makes sense.</p>

<p>However, I think things are a little more challenging in robotics. I may be an idealist: <strong>the best robot foundation models must be developed with view over the entire stack</strong>. Models must be built in tandem with hardware, software, data collection and deployment. That allows for iteration across all parts of the robot stack. What is ‚Äúhigh quality data‚Äù if it isn‚Äôt tailored to the specific form factor of the robot? How do we figure out things like occlusion (and do we need to)? Should we build in other senses like touch or sound into the robot or model? How should we think about the tokens required with three or so cameras?</p>

<p>Then comes deployment. The industry is rushing to collect data in diverse environments - which is great - but it is unclear how the ROI will be delivered. It is not yet clear how this diversity will translate to real world deployment and use. <strong>I believe that robot data collection is best if done in tandem with real world deployment, with companies that will actually use these robots</strong>. I‚Äôve had tons of very intelligent VCs and roboticists tell me we‚Äôre in the early days of AI robotics, much like the GPT-1 era, and use case is something that OpenAI only thought about much later. I don‚Äôt disagree. Though I think that if we consider what robots‚Äô GPT-4 looks like; learn from lessons in the LLM space; and build with use cases in mind; we will collect better data and deploy better robot models in the world.</p>

<p>Model, data and deployment must be a closed loop, ideally with real world partners willing to invest time to see the fruits of the labour. Unlike LLMs, where the public can be ‚Äútesters‚Äù, robots require manual deployment and physical iteration before the models can truly be deployed for real world use. It‚Äôs operationally tedious. But we can build processes and stacks to solve that. I think that‚Äôs the best way for robots to learn.</p>

<p>Happy New Year‚Äôs Eve, and wishing you a very blessed 2026 ahead.</p>

<p><strong>DNMZ</strong></p>

<p><img src="/assets/images/laundry_robots.gif" alt="Robot" />
My teleoperated robots, folding laundry in a facility in UK.</p>

  </div>
</article>

  <footer class="footer">
    <div class="container">
      <div class="social-links-circular">
        <a href="mailto:danielngmz@gmail.com" class="social-icon" aria-label="Email">
          <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect width="20" height="16" x="2" y="4" rx="2"/><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"/></svg>
        </a>
        
        <a href="https://www.linkedin.com/in/danielngmz/" target="_blank" class="social-icon" aria-label="LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"/><rect width="4" height="12" x="2" y="9"/><circle cx="4" cy="4" r="2"/></svg>
        </a>
  
        <a href="#" target="_blank" class="social-icon" aria-label="X">
          <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4l11.733 16h4.267l-11.733-16zM4 20l6.768-6.768M20 4l-6.768 6.768"/></svg>
        </a>
      </div>
      <div class="copyright">Daniel Ng ¬© 2026</div>
    </div>
  </footer>

  <script>
    (function () {
      const btn = document.getElementById("menuBtn");
      const menu = document.getElementById("mobileMenu");
      if (!btn || !menu) return;
  
      btn.addEventListener("click", () => {
        const open = menu.style.display === "block";
        menu.style.display = open ? "none" : "block";
        btn.setAttribute("aria-expanded", String(!open));
        menu.setAttribute("aria-hidden", String(open));
      });
  
      // Close on link click (mobile)
      menu.querySelectorAll("a").forEach((a) => {
        a.addEventListener("click", () => {
          menu.style.display = "none";
          btn.setAttribute("aria-expanded", "false");
          menu.setAttribute("aria-hidden", "true");
        });
      });
    })();
  </script>

</body>
</html>


